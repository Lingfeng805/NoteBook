- 《Deep Residual Learning for Image Recognition》（基于深度残差学习的图像识别）
- 《Identity Mappings in Deep Residual Networks》（深度残差网络中的特征映射）



在深度学习中，网络层数增多一般会伴随出现如下几个问题：

- 【1】计算资源的消耗增加；————（通过GPU集群解决）
- 【2】模型容易过拟合；————（通过海量数据、dropout正则化方法等解决）
- 【3】梯度消失/梯度爆炸问题的产生；————（通过[Batch Normalization](https://zhuanlan.zhihu.com/p/93643523)）

是否意味着只要通过无脑的增加网络的层数，就能使实验结果更为理想？:face_with_head_bandage:

随着网络层数的增加，网络发生了**退化现象**(`degradation`):随着网络层数的增多，训练集误差率逐渐下降，然后趋于饱和，当继续增加网络层数，训练集误差率**反而会增大**。

![111417_u90M_876354.png](E:/software/Typora/pictures/111417_u90M_876354.png)

当网络退化时，浅层网络能够达到比深层网络更好的训练效果；此时，若把浅层的特征传到深层，那么效果应该至少不比浅层的网络效果差（此时深层的输入特征=浅层的输入特征+深层自己获取的输入特征）；

## 1.残差网络

### 1.1 残差块

残差网络由一系列的残差块(图1)组成。一个残差块可表示为：$x_{l+1}=x_l+F(x_l,W_l)$   `(1)`

![img](E:/software/Typora/pictures/v2-bd76d0f10f84d74f90505eababd3d4a1_720w.jpg)

​                                                                              图1： 残差块

在卷积网络中，$x_l$可能和$x_{l+1}$的`Feature Map`数量不一样，此时需要使用`1×1`卷积进行升维或者降维，残差块表示为：$x_{l+1}=h(x_l)+F(x_l,W_l)$   `(2)`   其中，$h(x_l)=W'_lx$.而$W'_l$是`1×1`卷积操作。

![img](E:/software/Typora/pictures/v2-54d11fdb5da318615fae5f579f68c31a_720w.jpg)

​                                                                              图2： 1*1残差块

### 1.2 残差网络

残差网络的搭建：

- 【1】使用VGG公式搭建Plain VGG网络；
- 【2】在Plain VGG的卷积网络之间插入`Identity Mapping`，注意需要升维或者降维的时候加入`1×1`卷积。

在实现过程中，一般是直接堆叠残差块的方式：

```python
def resnet_v1(x):
    x = Conv2D(kernel_size=(3,3), filters=16, strides=1, padding='same', activation='relu')(x)
    x = res_block_v1(x, 16, 16)
    x = res_block_v1(x, 16, 32)
    x = Flatten()(x)
    outputs = Dense(10, activation='softmax', kernel_initializer='he_normal')(x)
    return outputs
```

### 1.3 为何叫残差网络

在统计学中，**误差**：衡量观测值和真实值之间的差距；**残差**：指预测值和观测值之间的差距；

网络的一层可看作：$y=H(x)$;

残差网络的一个残差块可表示为：$H(x)=F(x)+x$;  即$F(x)=H(x)-x$; 在单位映射中，`y=x`便是观测值，而$H(x)$是预测值，所以$F(x)$便对应着残差，因此叫做残差网络。



## 2. 残差网络的背后原理

残差块的一个更通用的表达方式：$y_l=h(x_l)+F(x_l,W_l)$  `(3)`

​															$x_{l+1}=f(y_l)$   `(4)`

暂不考虑升维或者降维，则$h(·)$为直接映射，$f(·)$为激活函数，一般用ReLU。

### 2.1 直接映射是最好的选择

$y_l=x_l+F(x_l,W_l)$  ``

$y_{l+1}=x_{l+1}+F(x_{l+1},w_{l+1})=f(y_l)+F(f(y_l),w_{l+1})$   `(4)`

### 2.2 激活函数的位置

![preview](E:/software/Typora/pictures/v2-1c02c8b95a7916ad759a98507fb26079_r.jpg)

![preview](E:/software/Typora/pictures/v2-ffe81dab49de5306fb001e1da7de7ce3_r.jpg)

**将激活函数移动到残差部分可以提高模型的精度。**

该网络一般就在resnet_v2，keras实现如下：

```python
def res_block_v2(x, input_filter, output_filter):
    res_x = BatchNormalization()(x)
    res_x = Activation('relu')(res_x)
    res_x = Conv2D(kernel_size=(3,3), filters=output_filter, strides=1, padding='same')(res_x)
    res_x = BatchNormalization()(res_x)
    res_x = Activation('relu')(res_x)
    res_x = Conv2D(kernel_size=(3,3), filters=output_filter, strides=1, padding='same')(res_x)
    if input_filter == output_filter:
        identity = x
    else: #需要升维或者降维
        identity = Conv2D(kernel_size=(1,1), filters=output_filter, strides=1, padding='same')(x)
    output= keras.layers.add([identity, res_x])
    return output

def resnet_v2(x):
    x = Conv2D(kernel_size=(3,3), filters=16 , strides=1, padding='same', activation='relu')(x)
    x = res_block_v2(x, 16, 16)
    x = res_block_v2(x, 16, 32)
    x = BatchNormalization()(x)
    y = Flatten()(x)
    outputs = Dense(10, activation='softmax', kernel_initializer='he_normal')(y)
    return outputs
```

